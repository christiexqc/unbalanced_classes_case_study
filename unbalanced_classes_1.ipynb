{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "pandas.set_option('display.max_columns', 20)\n",
    "pandas.set_option('display.width', 350)\n",
    "  \n",
    "#set seed to be able to reproduce the results\n",
    "np.random.seed(4684)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high high majority of real datasets are highly unbalanced. Typically, the minority class is between 1 and 3% for the most important metrics, i.e. conversion rate, ads click-through-rate, fraud, email click-rate.\n",
    "\n",
    "\n",
    "Building a model with unbalanced data presents a few challenges. The biggest problem is that if you predict everything as majority class, you will get a huge accuracy. And many models internally are built to optimize accuracy. So feed into the model a dataset with 2% class 1 and 98% class 0, and you will get as an output a model that predicts (almost) everything as class 0 with ~98% accuracy. 98% accuracy might look good at first, but a model like that is hardly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('./dataset/emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1\n",
      "0  64342  269\n",
      "1   1342   14\n"
     ]
    }
   ],
   "source": [
    "#get dummy variables from categorical ones\n",
    "data_dummy = pandas.get_dummies(data, drop_first=True).drop('email_id', axis=1)\n",
    "  \n",
    "#split into train and test to avoid overfitting\n",
    "train, test = train_test_split(data_dummy, test_size = 0.34)\n",
    "  \n",
    "#build the model. We choose a RF, but this issues applies to pretty much all models\n",
    "rf = RandomForestClassifier(n_estimators=50, oob_score=True)\n",
    "rf.fit(train.drop('clicked', axis=1), train['clicked'])\n",
    "#let's print OOB confusion matrix\n",
    "print(pandas.DataFrame(confusion_matrix(train['clicked'], rf.oob_decision_function_[:,1].round(), labels=[0, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1\n",
      "0  33126  144\n",
      "1    706    7\n"
     ]
    }
   ],
   "source": [
    "#and let's print test set confusion matrix\n",
    "print(pandas.DataFrame(confusion_matrix(test['clicked'], rf.predict(test.drop('clicked', axis=1)), labels=[0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let’s make sure we understand the confusion matrix output. The first one is for RF OOB errors and the second one is for the test set.\n",
    "\n",
    "\n",
    "Top left (where column == 0 and row == 0) are true negatives. These are the events that the model classifies as 0s and are indeed 0s. Expect this number to always be very large, after all it is the easy part of the problem, given how many 0s we have in the dataset\n",
    "\n",
    "\n",
    "Bottom left (where column == 0 and row == 1) are the events that the model classifies as 0, but are in fact 1s. It is crucial to minimize this as much as possible. If the model classifies everything as 0, this number will be large\n",
    "\n",
    "\n",
    "Column == 1 and row == 0, these are the events that are 0s, but the model classifies them as 1. At first, this number will be very low. Again, if it is classifying everything as 0, there will be very few events here. As you improve the model, this number will go up. While you should avoid making this number go up too much (after all they are still misclassifications), that’s part of the trade-off. It is fine to see this number increase in order to increase the number of events correctly classified as 1\n",
    "\n",
    "\n",
    "Column == 1 and row == 1, these are the true positives. Your goal should be increasing this number. And the only way to do it is by decreasing the number of events that the model classifies as 0, but are in fact 1s (column == 0 and row == 1)\n",
    "\n",
    "\n",
    "To summarizes all this, we can create class 0 and 1 errors. The first one is class error for class 0 and the other one for class 1. If the model classifies everything as majority class, class error for class 0 will be close to 0 (meaning it is perfect), but the other one will be close to 1 (meaning useless). Your goal is to decrease class 1 error, without increasing too much class 0 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_class0_error  test_class1_error\n",
      "0           0.004328           0.990182\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix test set\n",
    "conf_matrix = pandas.DataFrame(confusion_matrix(test['clicked'], rf.predict(test.drop('clicked', axis=1)), labels=[0, 1]))\n",
    "#class0/1 errors are 1 -  (correctly classified events/total events belonging to that class)\n",
    "class0_error = 1 - conf_matrix.loc[0,0]/(conf_matrix.loc[0,0]+conf_matrix.loc[0,1])\n",
    "class1_error = 1 - conf_matrix.loc[1,1]/(conf_matrix.loc[1,0]+conf_matrix.loc[1,1])\n",
    "  \n",
    "print(pandas.DataFrame( {'test_class0_error':[class0_error],\n",
    "                        'test_class1_error':[class1_error]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Play with model cut-off point\n",
    "\n",
    "So obviously the models we built above are useless. Our goal is to force the model to classify more events as class 1, even if this means losing overall accuracy.\n",
    "\n",
    "The simplest way we can try to achieve that is to simply change the model cut-off point. A couple of words about the cut-off point: every model, for each event, returns a probability between 0 and 1. Typically, model default cut-off value is 0.5, meaning that if that probability is lower than 0.5, the model predicts 0, else predicts 1. But you can change that cut-off value. In a RF, for R that probability is simply the proportion of trees that predict 1. So, if my RF has 50 trees and 10 of those predict 1, the final model score will be 0.2. For Python, it is the mean of all the probabilities returned by each tree.\n",
    "\n",
    "Right now, our model is using the default 0.5 cut-off value. So it feels pretty straightforward that, if we want to increase the number of events classified as class 1, we can just change the cut-off value accordingly. Maybe if we create a rule like: everything >= 0.1 will be class 1, we will improve our class 1 error.\n",
    "So, let’s see how modifying the cut-off point will change my model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "  \n",
    "#get test set predictions as a probability\n",
    "pred_prob=rf.predict_proba(test.drop('clicked', axis=1))[:,1]\n",
    "#get false positive rate and true positive rate, for different cut-off points\n",
    "#and let's save them in a dataset. \n",
    "fpr, tpr, thresholds = roc_curve(test['clicked'],pred_prob)\n",
    "# we will focus on class errors, defined as\n",
    "# class0_error = fpr and class1_error = 1 - tpr\n",
    "error_cutoff=pandas.DataFrame({'cutoff':pandas.Series(thresholds),\n",
    "                               'class0_error':pandas.Series(fpr),\n",
    "                               'class1_error': 1 - pandas.Series(tpr)\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cutoff  class0_error  class1_error  accuracy\n",
      "0     1.960000      0.000000      1.000000  0.979019\n",
      "1     0.960000      0.000030      1.000000  0.978989\n",
      "2     0.944000      0.000090      1.000000  0.978931\n",
      "3     0.900000      0.000120      1.000000  0.978901\n",
      "4     0.860000      0.000180      1.000000  0.978842\n",
      "...        ...           ...           ...       ...\n",
      "1178  0.001429      0.188127      0.674614  0.801666\n",
      "1179  0.001176      0.188157      0.674614  0.801636\n",
      "1180  0.001053      0.188218      0.674614  0.801577\n",
      "1181  0.000909      0.188248      0.674614  0.801548\n",
      "1182  0.000000      1.000000      0.000000  0.020981\n",
      "\n",
      "[1183 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#let's also add accuracy to the dataset, i.e. overall correctly classified events.\n",
    "#This is: (tpr * positives samples in the test set + tnr * positive samples in the dataset)/total_events_in_the_data_set\n",
    "error_cutoff['accuracy']=((1-error_cutoff['class0_error'])*sum(test['clicked']==0)+(1-error_cutoff['class1_error'])*sum(test['clicked']==1))/len(test['clicked'])\n",
    "  \n",
    "print(error_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand this output:\n",
    "\n",
    "\n",
    "Cutoff are the different cut-off values we are considering. It usually goes from 0 to 1. In this case, it goes from 0 to 0.96, everything above 0.96 (i.e. 0.97,0.98, etc.) will give the same classification as 0.96. The first row (1.96) doesn’t really matter. It just shows what happens when all events are predicted as majority class and python arbitrarily sets it to 1+max(cutoff)\n",
    "\n",
    "\n",
    "Class0_error is 1 - true negatives/all negative events. This is also called false negative rate. It simply means: of all class 0 events, how many can I correctly classify? Then take 1 minus that number\n",
    "\n",
    "\n",
    "Class1_error 1 - true positives/all positive events. This is also called false positive rate. It simply means: of all class 1 events, how many can I correctly classify? Then take 1 minus that number\n",
    "\n",
    "\n",
    "Accuracy. This is simply the model accuracy, so correctly classified events divided by all events.\n",
    "\n",
    "\n",
    "The most important thing of this table is understanding the trade-offs as I decrease the cut-off value. Decreasing the cut-off value leads to an increase in class0_error (which is bad), but an improvement in class1_error (which is good). The two extremes (when either class0_error = 0 and class1_error = 1 or the other way round) are obviously useless. It means just classifying all events as a given class. The point is finding the best combination of the two class errors.\n",
    "\n",
    "\n",
    "Also, as you can see, accuracy is largest when the model classifies everything as majority class. That’s obvious. If you have 98% majority class events, you get a great looking 98% accuracy by classifying everything as class 0. But that’s again useless. As you improve class1 error, you will see accuracy decrease. That’s ok, you are willing to accept that (not that you have any other options). But keep this trade-off in mind making sure accuracy doesn’t go down too much.\n",
    "\n",
    "\n",
    "The most common way to optimize class0 and class1 errors taking into account the trade-off is by maximizing this formula:\n",
    "\n",
    "\n",
    "(1-class1_error) - class0_error\n",
    "\n",
    "\n",
    "The maximum value of this is when both class errors are 0, which will never happen in real life. However, at its core, this simply means that I am willing to increase class0_error by a certain number as long as class1_error goes down by a larger number.\n",
    "\n",
    "\n",
    "Imagine my starting point is class0_error = 0 and class1_error = 1. The formula above returns 1-1-0 = 0, so it is very bad. If I decrease class1_error by 0.1, but class0_error has increased by only 0.05, that number becomes: 1-0.9-0.05 = 0.05, so it went up and am happy. Which makes sense given that the loss on one side was offset by a larger gain on the other side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cutoff  class0_error  class1_error  accuracy  optimal_value\n",
      "1172  0.002500      0.186444      0.676017  0.803284       0.137539\n",
      "1178  0.001429      0.188127      0.674614  0.801666       0.137258\n",
      "1179  0.001176      0.188157      0.674614  0.801636       0.137228\n",
      "1180  0.001053      0.188218      0.674614  0.801577       0.137168\n",
      "1181  0.000909      0.188248      0.674614  0.801548       0.137138\n",
      "1173  0.002222      0.186865      0.676017  0.802872       0.137118\n",
      "1174  0.002000      0.187136      0.676017  0.802607       0.136848\n",
      "1171  0.002857      0.185993      0.677419  0.803696       0.136587\n",
      "1175  0.001818      0.187647      0.676017  0.802107       0.136337\n",
      "1176  0.001667      0.187767      0.676017  0.801989       0.136216\n",
      "1177  0.001538      0.188037      0.676017  0.801724       0.135946\n",
      "1169  0.003125      0.185813      0.678822  0.803843       0.135365\n",
      "1170  0.003077      0.185873      0.678822  0.803784       0.135305\n",
      "1165  0.003636      0.184491      0.680224  0.805108       0.135285\n",
      "1166  0.003529      0.184551      0.680224  0.805050       0.135225\n",
      "1163  0.004000      0.183980      0.681627  0.805579       0.134394\n",
      "1164  0.003772      0.184010      0.681627  0.805550       0.134363\n",
      "1167  0.003333      0.185753      0.680224  0.803873       0.134023\n",
      "1168  0.003158      0.185783      0.680224  0.803843       0.133993\n",
      "1162  0.004444      0.181876      0.684432  0.807580       0.133692\n"
     ]
    }
   ],
   "source": [
    "#let's check best combination of class0 and class1 errors\n",
    "error_cutoff['optimal_value'] = 1 - error_cutoff['class1_error'] - error_cutoff['class0_error']\n",
    "  \n",
    "print(error_cutoff.sort_values('optimal_value', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, according to this, the best cut-off value is ~0.0025. Meaning that all events whose RF predicted probability is >= 0.0025, will be predicted as 1, else 0. That probability is the mean calculated across the probabilities returned by each tree.\n",
    "\n",
    "\n",
    "Keep in mind that that formula can be useful, but at the end of the day domain knowledge and a strong business sense are much more important that optimizing some formula. For instance, there might be cases in which the cost of a false positive is much larger than the cost of a false negative. In any case, it would be pretty straightforward to modify that formula accordingly.\n",
    "\n",
    "\n",
    "All that formula is saying is: I am willing to accept a decrease of X in class0_error as long as class1_error improves by more than X. You can quickly modify it. Imagine the cost of class1_error is much larger than class0_error. You could say something like: I am willing to accept a decrease of X in class0_error as long as class1_error improves by at least X/2. Mathematically, this would be like maximizing:\n",
    "\n",
    "\n",
    "(1-class1_error)*2 - class0_error\n",
    "\n",
    "\n",
    "So now a decrease in class1_error weighs twice as much as an increase in class0_error.\n",
    "\n",
    "\n",
    "Finally, let’s check that everything makes sense by going back to our original model and rebuild the confusion matrix with the new cut-off point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1\n",
      "0  27067  6203\n",
      "1    482   231\n"
     ]
    }
   ],
   "source": [
    "#we already have predicted probabilities from the previous step, i.e.\n",
    "#pred_prob=rf.predict_proba(test.drop('clicked', axis=1))[:,1]\n",
    "  \n",
    "#let's create a 0/1 vector according to the 0.002 cutoff\n",
    "best_cutoff = error_cutoff.sort_values('optimal_value', ascending=False)['cutoff'].values[0]\n",
    "predictions=np.where(pred_prob>=best_cutoff,1,0)\n",
    "#get confusion matrix for those predictions\n",
    "#confusion matrix test set\n",
    "conf_matrix_new = pandas.DataFrame(confusion_matrix(test['clicked'], predictions, labels=[0, 1]))\n",
    "print(conf_matrix_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cutoff  test_class0_error_new  test_class1_error_new\n",
      "0  0.0025               0.186444               0.676017\n"
     ]
    }
   ],
   "source": [
    "class0_error = 1 - conf_matrix_new.loc[0,0]/(conf_matrix_new.loc[0,0]+conf_matrix_new.loc[0,1])\n",
    "class1_error = 1 - conf_matrix_new.loc[1,1]/(conf_matrix_new.loc[1,0]+conf_matrix_new.loc[1,1])\n",
    "print(pandas.DataFrame( {'cutoff':[best_cutoff],\n",
    "                         'test_class0_error_new':[class0_error],\n",
    "                         'test_class1_error_new':[class1_error]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! As you can see, true/false positive/negative rates are exactly the same as in the error_cutoff dataset when the cut-off is 0.0025\n",
    "\n",
    "\n",
    "Pros and Cons\n",
    "Pros of using the cut-off value strategy with unbalanced data\n",
    "\n",
    "✓ The interpretation is very straightforward. Since one class is way less likely to happen than the other one, I just force the model to classify more events as minority class by lowering the classification threshold. You are not modifying the data distribution or, in general, doing any tranformation that leads to lose interpretability\n",
    "\n",
    "\n",
    "Cons of using the cut-off value strategy with unbalanced data\n",
    "\n",
    "✓ It is not always possible to use this approach. If your classes are highly unbalanced, your model might not find any way to separate them. That would lead to basically no model and all events being classified as majority class with probability 1. In that case, since all events have probability 1, there is no point in doing a cut-off analysis. More on this in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
